\documentclass{beamer}
\usetheme{Frankfurt}
\usepackage{color}
\usepackage{graphicx}
\usepackage[font=scriptsize,skip=1pt]{caption}
\usepackage{textpos}
\usepackage{multirow}
\usepackage{multicol}
%\usepackage{wrapfig}
\usepackage[labelformat=empty]{caption}
\usepackage[labelformat=empty,skip=0pt,justification=raggedright]{subcaption}
\usepackage{remreset}
\usepackage{multimedia}
\usepackage{mdframed}
\usepackage{natbib}
\usepackage[export]{adjustbox}

\definecolor{beaublue}{rgb}{0.74, 0.83, 0.9}
\definecolor{bubbles}{rgb}{0.91, 1.0, 1.0}
\definecolor{arsenic}{rgb}{0.23, 0.27, 0.29}
\definecolor{blizzardblue}{rgb}{0.67, 0.9, 0.93}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{auburn}{rgb}{0.43, 0.21, 0.1}
\definecolor{applegreen}{rgb}{0.55, 0.71, 0.0}
\definecolor{camel}{rgb}{0.76, 0.6, 0.42}
\definecolor{scarlet}{rgb}{1.0, 0.13, 0.0}
\definecolor{shamrockgreen}{rgb}{0.0, 0.62, 0.38}
\definecolor{tiffanyblue}{rgb}{0.04, 0.73, 0.71}
\definecolor{saffron}{rgb}{0.96, 0.77, 0.19}
\definecolor{aphids}{rgb}{0.85, 0.85, 0.23}
\definecolor{dummy}{rgb}{0.6, 0.6, 0.99}
\definecolor{larvae}{rgb}{0.53, 0.53, 0.21}

\setbeamercolor{section in head/foot}{fg=arsenic, bg=bubbles}
%\setbeamercolor{subsection in head/foot}{fg=black, bg=babyblue}
\setbeamercolor{frametitle}{fg=auburn, bg=white}
\setbeamercolor{upper separation line head}{bg=airforceblue}
\setbeamertemplate{section in head/foot}{\insertsectionhead}

\makeatletter
\@removefromreset{subsection}{section}
\makeatother
\setcounter{subsection}{1}


\graphicspath{{./Figures/}}

 \title{An introduction to Bayesian Data Analysis using STAN}
 \subtitle{}
 
 \author{Lionel Hertzog \& Maxime Dahirel}
 
 \date{BDA workshop, BES-Gf\"{O}, Necov joint meeting, XX December 2017, Gent}
 
 


\begin{document}
 
 \frame{\titlepage}
 

\begin{frame}
 \frametitle{\bf Structure of the workshop}
  \begin{enumerate}
   \item General introduction to Bayesian Data Analysis (circa. 30min)
   \item Examples of BDA workflow (circa. 30min)
   \item Small group discussion on specific themes (circa. 30min)
  \end{enumerate}

 
 \end{frame}
 
\begin{frame}
 \frametitle{\bf Structure of the talk}
  \begin{itemize}
   \item What is Bayesian Data Analysis?
   \item How to do Bayesian Data Analysis?
   \item Why do Bayesian Data Analysis?
  \end{itemize}

 
 \end{frame} 
 
 
\section*{What?} 
 
 \begin{frame}
  \frametitle{\bf Why do we do Stats?}
  
  Some elements of inference, how does it fits into the science workflow
  
  Scientists are intersted with understanding and explaining the processes structuring the world, build theories to synthetically represent what is happening.
  Data are then assembled/collected and one wants to extract the big world signal in the data, statistical inference provide this bridge between complex data and
  theories. 
  
  
  
 \end{frame}

  \begin{frame}
  \frametitle{\bf Big world / small world}
  
  Our models have limited scope and only give answers within their area of expertise, models will tend to give the best answers within their limited scope
  
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf Bayesian data analysis in one formula}
  
  P (hypothesis given data) is prop to P(data given hypothesis) * P(hypothesis) or
  Posterior  is prop to Likelihood * Prior
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf What is Likelihood}
  
  P(Data | Parameter), based on some probability distribution we get the probability of observing the data for specific parameter values
  some ecological examples: length/height of some animals
  
  Maximum likelihood extimate (minimize the sum of log-likelihoods), this is what we get in classical analysis
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf What is prior infos}
  
  This is something new, prior is also a probability distribution that represent our beliefs of the likely values of the parameters
  This is the information that is around in the literature or in the community before doing an analysis
  Example: want to explore plant growth or whatever with a meta-analysis
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf What is Posterior}
  
  The posterior is also a probability distribution, it quantifies the probability of parameter values knowing the data (the likelihood) and our expectation (prior)
  The impact of the data vs the expectation depends on the sample size, with low sample size the prior is relatively more important (little information in the data), as
  sample size grows the likelihood gets more and more weights, a simple example (maybe a shiny app ...)
  
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf It's all about uncertainty}
  
  One of the great advantage of BDA is that everything is uncertain, we get posterior \emph{distribution} for all parameters in the models so we know
  the uncertainty associated with all parameters in the model. Moreover we can interprete the posterior distribution as probability statement and
  can therefore more easily interprete the model outcome. Questions like: what is the probability that Group A as larger means than group B, or what is the 
  probability that the variation in individual A response is larger than the average variation are straightforward to answer.
  
 \end{frame}
 
  
\section{How?}

 \begin{frame}
  \frametitle{\bf Ways to fit bayesian models in R}
  
  Coding in probability language vs using wrappers
  STAN is a programming language in its own so can code the models direclty in it (some snippet code example of simple model)
  Knowing this we have the full flexibility to fit any model we want but we also have the full possibility that we make coding/interpretation
  errors that are not visible.
  There are a couple of R packages to transcribe the standard R formula synthax into STAN models: rstanarm and brms. With this option we are sure that the model
  code is correct and also optimized so it will certainly run faster than naive implementation directly in STAN. 
  
 \end{frame}

  \begin{frame}
  \frametitle{\bf The sampling}
  
  Blind man in the likelihood landscape, how do we effectively sample it
  
 \end{frame}

  \begin{frame}
  \frametitle{\bf The sampling in BUGS and JAGS}
  
  Metropolis-Hasting sampling, adv. and dis.
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf The sampling in STAN}
  
  Hamiltonian Monte Carlo, adv. and dis.
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf Elements of bayesian vocabulary}
  
  Chains, convergence, divergence, Rhat, n\_eff, tree depth
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf Model checking in Bayesian data analysis}
  
  convergence, n\_eff, chains, posterior predictive checks
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf Model comparison}
  
  LOOC, WAIC ...
  
 \end{frame}
 

\section{Why?}
 
  \begin{frame}
  \frametitle{\bf Embracing uncertainty}
  
  Everything is variable, all parameters come with (posterior) distribution, can interprete posterior sample
  as probabilities, flexibility to test any hypothesis building on these
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf Flexibility in model building}
  
  As long as you can write your likelihood function you can fit any model you like (similar to using MLE approach
  ie with bbmle)
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf Why do we do stats?}
  
  Bayesian data analysis give us relevant answers in terms of probability rather than weird answers refering to som null hypothesis in
  terms of frequency
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf No degrees of freedom}
  
  Fit complex models even with little data (is this something we want??)
  
 \end{frame}
 
  \begin{frame}
  \frametitle{\bf Asymptotic convergence, when bayesian and frequentist approach give similar answers}
  
  With infinite sample size the posterior distribution just reflect the likelihood, as sample size increases
  priors have dwindling effects
  
 \end{frame}
 
\end{document}

 \begin{frame}
  \frametitle{\bf }
  
 \end{frame}